{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ssm_em_forecast.py\n",
        "\n",
        "Advanced Time-Series Forecasting with State-Space Models, EM and optimization-based\n",
        "estimation of noise covariances. This updated module implements:\n",
        "\n",
        "1) A documented synthetic data generator producing a series with two distinct\n",
        "   seasonalities (weekly and yearly), changing trend regimes and optional regressors.\n",
        "2) A clear StateSpaceModel class defining state transition and observation\n",
        "   equations (local linear trend + trigonometric seasonal blocks).\n",
        "3) Two estimation strategies for noise covariances (R and diagonal Q):\n",
        "   - EM algorithm with closed-form M-steps (already implemented), and\n",
        "   - Direct maximum-likelihood optimization using `scipy.optimize.minimize` on\n",
        "     the Kalman-filter likelihood (with parameter transforms for positivity).\n",
        "4) Forecasting utilities and benchmark against a SARIMAX baseline.\n",
        "5) Automatic computation of RMSE, MAE, and MAPE and a final textual analysis\n",
        "   comparing complexity, interpretability, and forecasting performance.\n",
        "\n",
        "Run the file as a script for a complete demo and summary.\n",
        "\n",
        "Dependencies: numpy, scipy, pandas, statsmodels (optional for benchmarking)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.linalg import block_diag\n",
        "from scipy.optimize import minimize\n",
        "from typing import Optional, Tuple, Dict\n",
        "\n",
        "try:\n",
        "    import statsmodels.api as sm\n",
        "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "except Exception:\n",
        "    sm = None\n",
        "    SARIMAX = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_synthetic_series(n: int = 800,\n",
        "                              seed: int = 42,\n",
        "                              weekly_amp: float = 5.0,\n",
        "                              yearly_amp: float = 20.0,\n",
        "                              noise_std: float = 3.0) -> Tuple[np.ndarray, Optional[np.ndarray], Dict]:\n",
        "    \"\"\"\n",
        "    Generate a synthetic time series with the following documented construction:\n",
        "\n",
        "    - Length n (e.g., 800 observations ~ multiple years of daily data)\n",
        "    - Two seasonalities: weekly (P=7) and yearly (P=365) implemented as sums of\n",
        "      harmonics (sin/cos terms). This yields smooth trigonometric seasonalities\n",
        "      that are easy to represent in a state-space form.\n",
        "    - Changing trend regimes: piecewise-linear trend with a break point in the\n",
        "      middle of the series to introduce non-stationarity / structural change.\n",
        "    - Additive Gaussian noise.\n",
        "    - An optional time-varying regressor (e.g., a marketing campaign indicator\n",
        "      or exogenous numeric) to make the identification problem more realistic.\n",
        "\n",
        "    Returns:\n",
        "      y: (n,) time series\n",
        "      X: (n,1) regressor array\n",
        "      meta: metadata dict containing components and parameters\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    t = np.arange(n)\n",
        "\n",
        "    # Piecewise linear trend: before and after a change point\n",
        "    cp = int(n * 0.5)\n",
        "    trend = np.zeros(n)\n",
        "    slope1 = 0.02\n",
        "    slope2 = 0.15\n",
        "    trend[:cp] = slope1 * t[:cp]\n",
        "    trend[cp:] = slope1 * cp + slope2 * (t[cp:] - cp)\n",
        "\n",
        "    # Weekly seasonality (period 7) - two harmonics\n",
        "    P_week = 7\n",
        "    weekly = np.zeros(n)\n",
        "    for h in [1, 2]:\n",
        "        weekly += (weekly_amp / h) * np.sin(2 * np.pi * h * t / P_week)\n",
        "\n",
        "    # Yearly seasonality (period 365) - a few harmonics but with smaller amplitude\n",
        "    P_year = 365\n",
        "    yearly = np.zeros(n)\n",
        "    for h in [1, 2, 3]:\n",
        "        yearly += (yearly_amp / h) * np.sin(2 * np.pi * h * t / P_year)\n",
        "\n",
        "    # Regressor: slowly varying numeric with occasional shocks (e.g., campaigns)\n",
        "    X = rng.normal(scale=1.0, size=(n, 1))\n",
        "    campaign = np.zeros(n)\n",
        "    # place 3 campaigns at random epochs\n",
        "    for start in [int(n*0.2), int(n*0.5), int(n*0.8)]:\n",
        "        campaign[start:start+10] += 3.0\n",
        "    X[:, 0] += campaign\n",
        "\n",
        "    true_beta = np.array([2.5])\n",
        "    reg_effect = X.dot(true_beta)\n",
        "\n",
        "    noise = rng.normal(scale=noise_std, size=n)\n",
        "\n",
        "    y = trend + weekly + yearly + reg_effect.flatten() + noise\n",
        "\n",
        "    meta = {\n",
        "        'trend': trend,\n",
        "        'weekly': weekly,\n",
        "        'yearly': yearly,\n",
        "        'regressor': X,\n",
        "        'true_beta': true_beta,\n",
        "        'noise_std': noise_std,\n",
        "        'change_point': cp,\n",
        "        'periods': {'weekly': P_week, 'yearly': P_year}\n",
        "    }\n",
        "    return y, X, meta\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class StateSpaceModel:\n",
        "    \"\"\"\n",
        "    Linear Gaussian State-Space Model with explicit local linear trend and\n",
        "    trigonometric seasonal components implemented as state blocks.\n",
        "\n",
        "    State vector alpha_t contains:\n",
        "      [level, slope, seasonal_week_cos, seasonal_week_sin, seasonal_year_cos_1, seasonal_year_sin_1, ...]\n",
        "\n",
        "    Observation equation:\n",
        "      y_t = Z alpha_t + x_t' beta + epsilon_t,  epsilon_t ~ N(0, R)\n",
        "\n",
        "    State transition:\n",
        "      alpha_{t+1} = T alpha_t + eta_t,  eta_t ~ N(0, Q)\n",
        "\n",
        "    The trigonometric seasonal components use the standard rotation matrix per\n",
        "    harmonic (2x2 blocks) so they are parsimonious and smooth.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_week_harmonics: int = 1,\n",
        "                 n_year_harmonics: int = 3,\n",
        "                 k_regressors: int = 1):\n",
        "\n",
        "        self.n_week_harmonics = n_week_harmonics\n",
        "        self.n_year_harmonics = n_year_harmonics\n",
        "        self.k_regressors = k_regressors\n",
        "\n",
        "        # Indexing\n",
        "        pos = 0\n",
        "        self.idx = {}\n",
        "        self.idx['level'] = pos; pos += 1\n",
        "        self.idx['slope'] = pos; pos += 1\n",
        "\n",
        "\n",
        "        self.idx['weekly'] = (pos, pos + 2 * n_week_harmonics)\n",
        "        pos += 2 * n_week_harmonics\n",
        "\n",
        "\n",
        "        self.idx['yearly'] = (pos, pos + 2 * n_year_harmonics)\n",
        "        pos += 2 * n_year_harmonics\n",
        "\n",
        "        self.m = pos\n",
        "\n",
        "\n",
        "        self.R = 1.0\n",
        "\n",
        "        self.Q_diag = np.ones(self.m) * 0.01\n",
        "        self.beta = np.zeros(k_regressors) if k_regressors > 0 else None\n",
        "\n",
        "\n",
        "        self.T = np.eye(self.m)\n",
        "        self.Z = np.zeros((1, self.m))\n",
        "        self._build_matrices()\n",
        "\n",
        "    def _build_matrices(self):\n",
        "        m = self.m\n",
        "        T = np.eye(m)\n",
        "\n",
        "\n",
        "        T[self.idx['level'], self.idx['level']] = 1.0\n",
        "        T[self.idx['level'], self.idx['slope']] = 1.0\n",
        "\n",
        "        T[self.idx['slope'], self.idx['slope']] = 1.0\n",
        "\n",
        "        # weekly harmonic blocks\n",
        "        wk_start, wk_end = self.idx['weekly']\n",
        "        for h in range(self.n_week_harmonics):\n",
        "            k = h + 1\n",
        "            omega = 2 * np.pi * k / 7.0\n",
        "            cos = np.cos(omega); sin = np.sin(omega)\n",
        "            block = np.array([[cos, -sin],[sin, cos]])\n",
        "            i = wk_start + 2*h\n",
        "            T[i:i+2, i:i+2] = block\n",
        "\n",
        "        # yearly harmonic blocks\n",
        "        yr_start, yr_end = self.idx['yearly']\n",
        "        for h in range(self.n_year_harmonics):\n",
        "            k = h + 1\n",
        "            omega = 2 * np.pi * k / 365.0\n",
        "            cos = np.cos(omega); sin = np.sin(omega)\n",
        "            block = np.array([[cos, -sin],[sin, cos]])\n",
        "            i = yr_start + 2*h\n",
        "            T[i:i+2, i:i+2] = block\n",
        "\n",
        "        self.T = T\n",
        "\n",
        "        # Observation matrix Z: level plus sum of cos components map to observation\n",
        "        Z = np.zeros((1, m))\n",
        "        Z[0, self.idx['level']] = 1.0\n",
        "\n",
        "\n",
        "        wk_start, wk_end = self.idx['weekly']\n",
        "        for i in range(wk_start, wk_end, 2):\n",
        "            Z[0, i] = 1.0\n",
        "        yr_start, yr_end = self.idx['yearly']\n",
        "        for i in range(yr_start, yr_end, 2):\n",
        "            Z[0, i] = 1.0\n",
        "\n",
        "        self.Z = Z\n",
        "\n",
        "    # Kalman filter returns prediction errors for likelihood computation\n",
        "    def kalman_filter(self, y: np.ndarray, X: Optional[np.ndarray] = None) -> Dict:\n",
        "        n = len(y)\n",
        "        m = self.m\n",
        "        R = float(self.R)\n",
        "        Q = np.diag(self.Q_diag)\n",
        "\n",
        "        a_prev = np.zeros(m)\n",
        "        P_prev = np.eye(m) * 1e6\n",
        "\n",
        "        v = np.zeros(n)\n",
        "        F = np.zeros(n)\n",
        "        a_pred = np.zeros((n, m))\n",
        "        P_pred = np.zeros((n, m, m))\n",
        "        a_upd = np.zeros((n, m))\n",
        "        P_upd = np.zeros((n, m, m))\n",
        "\n",
        "        for t in range(n):\n",
        "            # predict\n",
        "            a_pr = self.T.dot(a_prev)\n",
        "            P_pr = self.T.dot(P_prev).dot(self.T.T) + Q\n",
        "\n",
        "            # observation\n",
        "            d_t = 0.0\n",
        "            if X is not None and self.beta is not None:\n",
        "                d_t = float(X[t].dot(self.beta))\n",
        "            v_t = y[t] - float(self.Z.dot(a_pr) + d_t)\n",
        "            F_t = float(self.Z.dot(P_pr).dot(self.Z.T) + R)\n",
        "\n",
        "            K = P_pr.dot(self.Z.T) / F_t\n",
        "            a_up = a_pr + K.flatten() * v_t\n",
        "            P_up = P_pr - K.dot(self.Z).dot(P_pr)\n",
        "\n",
        "            v[t] = v_t\n",
        "            F[t] = F_t\n",
        "            a_pred[t] = a_pr\n",
        "            P_pred[t] = P_pr\n",
        "            a_upd[t] = a_up\n",
        "            P_upd[t] = P_up\n",
        "\n",
        "            a_prev = a_up\n",
        "            P_prev = P_up\n",
        "\n",
        "        return {'v': v, 'F': F, 'a_pred': a_pred, 'P_pred': P_pred, 'a_upd': a_upd, 'P_upd': P_upd}\n",
        "\n",
        "    def log_likelihood(self, y: np.ndarray, X: Optional[np.ndarray] = None) -> float:\n",
        "        res = self.kalman_filter(y, X)\n",
        "        v = res['v']; F = res['F']\n",
        "        # compute log-likelihood (Gaussian): sum(-0.5*(log(2pi) + log F + v^2/F))\n",
        "        ll = -0.5 * np.sum(np.log(2 * np.pi) + np.log(F) + (v**2) / F)\n",
        "        return ll\n",
        "\n",
        "    # EM step (as before) retained for comparison\n",
        "    def em_step(self, y: np.ndarray, X: Optional[np.ndarray] = None) -> None:\n",
        "\n",
        "        kf = self.kalman_filter(y, X)\n",
        "\n",
        "        a_s = kf['a_upd']\n",
        "        P_s = kf['P_upd']\n",
        "\n",
        "\n",
        "        n = len(y)\n",
        "        resid = 0.0\n",
        "        for t in range(n):\n",
        "            mean_y = float(self.Z.dot(a_s[t]) + (X[t].dot(self.beta) if (X is not None and self.beta is not None) else 0.0))\n",
        "            resid += (y[t] - mean_y)**2 + float(self.Z.dot(P_s[t]).dot(self.Z.T))\n",
        "        self.R = float(resid / n)\n",
        "\n",
        "        # Update Q diagonal using state innovations approx\n",
        "\n",
        "        Q_acc = np.zeros(self.m)\n",
        "        for t in range(n - 1):\n",
        "            e = a_s[t+1] - self.T.dot(a_s[t])\n",
        "            Q_acc += e**2 + np.diag(P_s[t+1])\n",
        "        self.Q_diag = np.maximum(Q_acc / max(1, n - 1), 1e-8)\n",
        "\n",
        "        # Beta via OLS on residuals\n",
        "        if X is not None and self.k_regressors > 0:\n",
        "            y_tilde = np.zeros(n)\n",
        "            for t in range(n):\n",
        "                y_tilde[t] = y[t] - float(self.Z.dot(a_s[t]))\n",
        "            XtX = X.T.dot(X)\n",
        "            Xty = X.T.dot(y_tilde)\n",
        "            try:\n",
        "                self.beta = np.linalg.solve(XtX, Xty)\n",
        "            except np.linalg.LinAlgError:\n",
        "                self.beta = np.linalg.lstsq(XtX, Xty, rcond=None)[0]\n",
        "\n",
        "    def fit_em(self, y: np.ndarray, X: Optional[np.ndarray] = None, n_iter: int = 30, tol: float = 1e-6, verbose: bool = True):\n",
        "        prev = np.hstack([np.atleast_1d(self.R), self.Q_diag])\n",
        "        history = {'R': [], 'Q_diag': []}\n",
        "        for it in range(n_iter):\n",
        "            self.em_step(y, X)\n",
        "            history['R'].append(self.R)\n",
        "            history['Q_diag'].append(self.Q_diag.copy())\n",
        "            cur = np.hstack([np.atleast_1d(self.R), self.Q_diag])\n",
        "            rel = np.max(np.abs(cur - prev) / (np.abs(prev) + 1e-8))\n",
        "            if verbose:\n",
        "                print(f\"EM iter {it+1}: R={self.R:.4f}, Q_mean={np.mean(self.Q_diag):.4e}, rel={rel:.2e}\")\n",
        "            if rel < tol:\n",
        "                break\n",
        "            prev = cur\n",
        "        return history\n",
        "\n",
        "    # Optimization-based estimation of noise parameters (MLE)\n",
        "    def mle_estimate_QR(self, y: np.ndarray, X: Optional[np.ndarray] = None,\n",
        "                        method: str = 'L-BFGS-B', verbose: bool = True) -> Dict:\n",
        "        \"\"\"\n",
        "        Estimate parameters theta = [log(R), log(Q_diag_1), ..., log(Q_diag_m)] by\n",
        "        maximizing the Kalman-filter log-likelihood (equivalently minimizing negative log-likelihood).\n",
        "\n",
        "        We optimize in log-space to enforce positivity.\n",
        "        \"\"\"\n",
        "        m = self.m\n",
        "        # initial guess from current R and Q_diag\n",
        "        x0 = np.hstack([np.log(max(self.R, 1e-8)), np.log(np.maximum(self.Q_diag, 1e-9))])\n",
        "\n",
        "        def obj(x):\n",
        "            logR = x[0]\n",
        "            logQ = x[1:]\n",
        "            self.R = float(np.exp(logR))\n",
        "            self.Q_diag = np.exp(logQ)\n",
        "            ll = self.log_likelihood(y, X)\n",
        "            if np.isnan(ll) or np.isinf(ll):\n",
        "                return 1e12\n",
        "            return -ll\n",
        "\n",
        "        bounds = [(-20, 20)] * (1 + m)\n",
        "        res = minimize(obj, x0, method=method, bounds=bounds, options={'disp': verbose})\n",
        "\n",
        "        # assign final\n",
        "        if res.success:\n",
        "            self.R = float(np.exp(res.x[0]))\n",
        "            self.Q_diag = np.exp(res.x[1:])\n",
        "        else:\n",
        "            # still assign best known\n",
        "            self.R = float(np.exp(res.x[0]))\n",
        "            self.Q_diag = np.exp(res.x[1:])\n",
        "        return {'success': res.success, 'message': res.message, 'fun': res.fun, 'x': res.x}\n",
        "\n",
        "    # Forecast function using last filtered state\n",
        "    def forecast(self, steps: int, a_last: Optional[np.ndarray], P_last: Optional[np.ndarray], X_future: Optional[np.ndarray] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        preds = np.zeros(steps)\n",
        "        preds_var = np.zeros(steps)\n",
        "        a = a_last.copy()\n",
        "        P = P_last.copy()\n",
        "        Q = np.diag(self.Q_diag)\n",
        "        R = float(self.R)\n",
        "        for h in range(steps):\n",
        "            a = self.T.dot(a)\n",
        "            P = self.T.dot(P).dot(self.T.T) + Q\n",
        "            d = 0.0\n",
        "            if X_future is not None and self.beta is not None:\n",
        "                d = float(X_future[h].dot(self.beta))\n",
        "            preds[h] = float(self.Z.dot(a) + d)\n",
        "            preds_var[h] = float(self.Z.dot(P).dot(self.Z.T) + R)\n",
        "        return preds, preds_var\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict:\n",
        "    eps = 1e-8\n",
        "    rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
        "    mae = np.mean(np.abs(y_true - y_pred))\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + eps))) * 100.0\n",
        "    return {'RMSE': rmse, 'MAE': mae, 'MAPE_pct': mape}\n",
        "\n",
        "\n",
        "def train_test_split(y: np.ndarray, X: Optional[np.ndarray], test_size: int = 120):\n",
        "    n = len(y)\n",
        "    train_n = n - test_size\n",
        "    return y[:train_n], y[train_n:], (X[:train_n] if X is not None else None), (X[train_n:] if X is not None else None)\n",
        "\n",
        "\n",
        "def benchmark_pipeline():\n",
        "    # generate data\n",
        "    y, X, meta = generate_synthetic_series(n=800)\n",
        "    y_train, y_test, X_train, X_test = train_test_split(y, X, test_size=120)\n",
        "\n",
        "    # build model\n",
        "    model = StateSpaceModel(n_week_harmonics=1, n_year_harmonics=3, k_regressors=X.shape[1])\n",
        "\n",
        "    # initialize parameters sensibly\n",
        "    model.R = np.var(y_train) * 0.5\n",
        "    model.Q_diag = np.ones(model.m) * 0.001\n",
        "\n",
        "    #  Estimate via EM\n",
        "    print('Estimating parameters with EM...')\n",
        "    em_model = StateSpaceModel(n_week_harmonics=1, n_year_harmonics=3, k_regressors=X.shape[1])\n",
        "    em_model.R = model.R\n",
        "    em_model.Q_diag = model.Q_diag.copy()\n",
        "    em_model.beta = np.zeros(X.shape[1])\n",
        "    em_history = em_model.fit_em(y_train, X_train, n_iter=30, verbose=True)\n",
        "\n",
        "    # get last filtered state to forecast\n",
        "    kf_res = em_model.kalman_filter(y_train, X_train)\n",
        "    a_last = kf_res['a_upd'][-1]\n",
        "    P_last = kf_res['P_upd'][-1]\n",
        "    preds_em, var_em = em_model.forecast(len(y_test), a_last, P_last, X_future=X_test)\n",
        "    metrics_em = metrics(y_test, preds_em)\n",
        "    print('EM metrics:', metrics_em)\n",
        "\n",
        "    #  Estimate via MLE optimizer\n",
        "    print('Estimating parameters with MLE (optimization)...')\n",
        "    mle_model = StateSpaceModel(n_week_harmonics=1, n_year_harmonics=3, k_regressors=X.shape[1])\n",
        "    mle_model.R = model.R\n",
        "    mle_model.Q_diag = model.Q_diag.copy()\n",
        "    mle_model.beta = np.zeros(X.shape[1])\n",
        "    mle_res = mle_model.mle_estimate_QR(y_train, X_train, method='L-BFGS-B', verbose=False)\n",
        "    print('MLE optimization success:', mle_res['success'], mle_res.get('message',''))\n",
        "    kf_res2 = mle_model.kalman_filter(y_train, X_train)\n",
        "    a_last2 = kf_res2['a_upd'][-1]\n",
        "    P_last2 = kf_res2['P_upd'][-1]\n",
        "    preds_mle, var_mle = mle_model.forecast(len(y_test), a_last2, P_last2, X_future=X_test)\n",
        "    metrics_mle = metrics(y_test, preds_mle)\n",
        "    print('MLE metrics:', metrics_mle)\n",
        "\n",
        "    #  SARIMAX baseline\n",
        "    sar_metrics = None\n",
        "    if SARIMAX is not None:\n",
        "        print('Fitting SARIMAX baseline...')\n",
        "        try:\n",
        "\n",
        "            sar = SARIMAX(y_train, exog=X_train, order=(1,1,0), seasonal_order=(1,0,0,7), enforce_stationarity=False, enforce_invertibility=False)\n",
        "            sar_res = sar.fit(disp=False)\n",
        "            sar_pred = sar_res.get_forecast(steps=len(y_test), exog=X_test).predicted_mean\n",
        "            sar_metrics = metrics(y_test, sar_pred)\n",
        "            print('SARIMAX metrics:', sar_metrics)\n",
        "        except Exception as e:\n",
        "            print('SARIMAX failed:', e)\n",
        "\n",
        "    # Collate results\n",
        "    results = {\n",
        "        'meta': meta,\n",
        "        'em': {'model': em_model, 'metrics': metrics_em, 'history': em_history},\n",
        "        'mle': {'model': mle_model, 'metrics': metrics_mle, 'opt_result': mle_res},\n",
        "        'sarimax': {'metrics': sar_metrics}\n",
        "    }\n",
        "\n",
        "    # Textual analysis (brief â€” printed and returned)\n",
        "    analysis = []\n",
        "    analysis.append('Analysis of models:')\n",
        "    # Complexity\n",
        "    analysis.append('- Complexity: The custom SSM (EM or MLE) explicitly models level, slope, and harmonics via compact state-vector. Parameter count scales with number of harmonics and Q diagonal size. SARIMAX has fewer explicit components but often requires manual seasonal order selection and differencing.')\n",
        "    # Interpretability\n",
        "    analysis.append('- Interpretability: SSM provides interpretable state components (trend, seasonal harmonics, regressor effects). EM yields smoothed state trajectories useful for diagnostics. SARIMAX coefficients are less directly decomposed into smooth trend + harmonic seasonality.')\n",
        "    # Performance\n",
        "    analysis.append(f\"- Forecast performance (on this synthetic data): EM RMSE={metrics_em['RMSE']:.3f}, MLE RMSE={metrics_mle['RMSE']:.3f}, SARIMAX RMSE={(sar_metrics['RMSE'] if sar_metrics is not None else float('nan')):.3f}.\")\n",
        "    analysis.append('- MLE directly optimizes the likelihood and can give slightly better fit than EM in some cases, but is more sensitive to initialization and may be slower for large state dims. EM is simple and stable but can converge slowly.')\n",
        "\n",
        "    print(''.join(analysis))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    results = benchmark_pipeline()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDrkYP33EMSd",
        "outputId": "cc5fb0fe-3028-4470-93f3-7459bdd3047a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimating parameters with EM...\n",
            "EM iter 1: R=46.3273, Q_mean=7.5436e+04, rel=1.95e+08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4050946308.py:243: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  v_t = y[t] - float(self.Z.dot(a_pr) + d_t)\n",
            "/tmp/ipython-input-4050946308.py:244: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  F_t = float(self.Z.dot(P_pr).dot(self.Z.T) + R)\n",
            "/tmp/ipython-input-4050946308.py:281: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  mean_y = float(self.Z.dot(a_s[t]) + (X[t].dot(self.beta) if (X is not None and self.beta is not None) else 0.0))\n",
            "/tmp/ipython-input-4050946308.py:282: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  resid += (y[t] - mean_y)**2 + float(self.Z.dot(P_s[t]).dot(self.Z.T))\n",
            "/tmp/ipython-input-4050946308.py:297: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  y_tilde[t] = y[t] - float(self.Z.dot(a_s[t]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EM iter 2: R=46.3236, Q_mean=1.9243e+07, rel=3.51e+02\n",
            "EM iter 3: R=46.3235, Q_mean=3.3637e+09, rel=1.85e+02\n",
            "EM iter 4: R=46.3235, Q_mean=5.2492e+11, rel=1.64e+02\n",
            "EM iter 5: R=46.3204, Q_mean=7.8208e+13, rel=1.60e+02\n",
            "EM iter 6: R=46.7117, Q_mean=1.1389e+16, rel=1.55e+02\n",
            "EM iter 7: R=62.6441, Q_mean=1.6383e+18, rel=1.51e+02\n",
            "EM iter 8: R=-20.0475, Q_mean=2.3403e+20, rel=1.48e+02\n",
            "EM iter 9: R=-59608.2059, Q_mean=3.3294e+22, rel=2.97e+03\n",
            "EM iter 10: R=-39395616.0000, Q_mean=4.7242e+24, rel=6.60e+02\n",
            "EM iter 11: R=11341371319.7176, Q_mean=6.6922e+26, rel=2.89e+02\n",
            "EM iter 12: R=864706465165.5530, Q_mean=9.4699e+28, rel=1.42e+02\n",
            "EM iter 13: R=-270378664213564.2500, Q_mean=1.3391e+31, rel=3.14e+02\n",
            "EM iter 14: R=-10107438500505962.0000, Q_mean=1.8926e+33, rel=1.41e+02\n",
            "EM iter 15: R=-2744012440760403456.0000, Q_mean=2.6740e+35, rel=2.70e+02\n",
            "EM iter 16: R=408983810077586620416.0000, Q_mean=3.7772e+37, rel=1.50e+02\n",
            "EM iter 17: R=104349389283608116592640.0000, Q_mean=5.3348e+39, rel=2.54e+02\n",
            "EM iter 18: R=21542881081895738016792576.0000, Q_mean=7.5338e+41, rel=2.05e+02\n",
            "EM iter 19: R=-4589793363531128592173891584.0000, Q_mean=1.0639e+44, rel=2.14e+02\n",
            "EM iter 20: R=305194075184686219788531269632.0000, Q_mean=1.5022e+46, rel=1.40e+02\n",
            "EM iter 21: R=-74163318923940426329309262643200.0000, Q_mean=2.1211e+48, rel=2.44e+02\n",
            "EM iter 22: R=-1595497930960293139469066544611328.0000, Q_mean=2.9948e+50, rel=1.40e+02\n",
            "EM iter 23: R=790820639183959680512039782822445056.0000, Q_mean=4.2285e+52, rel=4.97e+02\n",
            "EM iter 24: R=-127564443235417579137180281440936394752.0000, Q_mean=5.9702e+54, rel=1.62e+02\n",
            "EM iter 25: R=-18119774407864147679061455303665243914240.0000, Q_mean=8.4292e+56, rel=1.41e+02\n",
            "EM iter 26: R=1841756660181224494827794920311666677645312.0000, Q_mean=1.1901e+59, rel=1.40e+02\n",
            "EM iter 27: R=-220442599666232416230730892731399061559574528.0000, Q_mean=1.6803e+61, rel=1.40e+02\n",
            "EM iter 28: R=41221492580785738777299870825096261114643087360.0000, Q_mean=2.3724e+63, rel=1.88e+02\n",
            "EM iter 29: R=10166401377405224761441799686897981741608080506880.0000, Q_mean=3.3495e+65, rel=2.46e+02\n",
            "EM iter 30: R=-572076367528436413264968045845689904907226944897024.0000, Q_mean=4.7290e+67, rel=1.40e+02\n",
            "EM metrics: {'RMSE': np.float64(9.956338836603408), 'MAE': np.float64(8.777602238730578), 'MAPE_pct': np.float64(21.209329168447912)}\n",
            "Estimating parameters with MLE (optimization)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4050946308.py:371: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  preds[h] = float(self.Z.dot(a) + d)\n",
            "/tmp/ipython-input-4050946308.py:372: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  preds_var[h] = float(self.Z.dot(P).dot(self.Z.T) + R)\n",
            "/tmp/ipython-input-4050946308.py:345: DeprecationWarning: scipy.optimize: The `disp` and `iprint` options of the L-BFGS-B solver are deprecated and will be removed in SciPy 1.18.0.\n",
            "  res = minimize(obj, x0, method=method, bounds=bounds, options={'disp': verbose})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLE optimization success: True CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH\n",
            "MLE metrics: {'RMSE': np.float64(5.139841346826129), 'MAE': np.float64(4.235163374973573), 'MAPE_pct': np.float64(8.6623656897369)}\n",
            "Fitting SARIMAX baseline...\n",
            "SARIMAX metrics: {'RMSE': np.float64(45.512737165509265), 'MAE': np.float64(37.73761211573125), 'MAPE_pct': np.float64(51.01889880119007)}\n",
            "Analysis of models:- Complexity: The custom SSM (EM or MLE) explicitly models level, slope, and harmonics via compact state-vector. Parameter count scales with number of harmonics and Q diagonal size. SARIMAX has fewer explicit components but often requires manual seasonal order selection and differencing.- Interpretability: SSM provides interpretable state components (trend, seasonal harmonics, regressor effects). EM yields smoothed state trajectories useful for diagnostics. SARIMAX coefficients are less directly decomposed into smooth trend + harmonic seasonality.- Forecast performance (on this synthetic data): EM RMSE=9.956, MLE RMSE=5.140, SARIMAX RMSE=45.513.- MLE directly optimizes the likelihood and can give slightly better fit than EM in some cases, but is more sensitive to initialization and may be slower for large state dims. EM is simple and stable but can converge slowly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kBV2JfqeEMaA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}